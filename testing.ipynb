{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Libraries \n",
    "\n",
    "import nltk, random, json , pickle\n",
    "#nltk.download('punkt');nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer=WordNetLemmatizer()\n",
    "context={};\n",
    "class Testing:\n",
    "    def __init__(self):\n",
    "        #load the intent file\n",
    "        self.intents = json.loads(open('intents.json').read())\n",
    "        #load the training_data file which contains training data\n",
    "        data=pickle.load(open(\"training_data\",\"rb\"))\n",
    "        self.words=data['words']\n",
    "        self.classes=data['classes']\n",
    "        self.model=load_model('chatbot_model.h5')\n",
    "        #set the error threshold value\n",
    "        self.ERROR_THRESHOLD=0.5\n",
    "        self.ignore_words=list(\"!@#$%^&*?\")\n",
    "        \n",
    "    def clean_up_sentence(self,sentence):\n",
    "        #tokenize each sentence (user's query)\n",
    "        sentence_words=word_tokenize(sentence.lower())\n",
    "        #lemmatize the word to root word and filter symbols words\n",
    "        sentence_words=list(map(lemmatizer.lemmatize,sentence_words))\n",
    "        sentence_words=list(filter(lambda x:x not in self.ignore_words,sentence_words))\n",
    "        return set(sentence_words)\n",
    "\n",
    "    def wordvector(self,sentence):\n",
    "        #'initialize CountVectorizer\n",
    "        #txt.split helps to tokenize single character\n",
    "        cv=CountVectorizer(tokenizer=lambda txt: txt.split())\n",
    "        sentence_words=' '.join(self.clean_up_sentence(sentence))\n",
    "        words=' '.join(self.words)\n",
    "\n",
    "        #fit the words into cv and transform into one-hot encoded vector\n",
    "        vectorize=cv.fit([words])\n",
    "        word_vector=vectorize.transform([sentence_words]).toarray().tolist()[0]\n",
    "        return(np.array(word_vector)) \n",
    "\n",
    "    def classify(self,sentence):\n",
    "        #predict to which class(tag) user's query belongs to\n",
    "        results=self.model.predict(np.array([self.wordvector(sentence)]))[0]\n",
    "        #store the class name and probability of that class \n",
    "        results = list(map(lambda x: [x[0],x[1]], enumerate(results)))\n",
    "        #accept those class probability which are greater then threshold value,0.5\n",
    "        results = list(filter(lambda x: x[1]>self.ERROR_THRESHOLD ,results))\n",
    "\n",
    "        #sort class probability value in descending order\n",
    "        results.sort(key=lambda x: x[1], reverse=True)\n",
    "        return_list = []\n",
    "\n",
    "        for i in results:\n",
    "            return_list.append((self.classes[i[0]],str(i[1])))\n",
    "        return return_list\n",
    "    \n",
    "    def results(self,sentence,userID):\n",
    "        #if context is maintained then filter class(tag) accordingly\n",
    "        if sentence.isdecimal():\n",
    "            if context[userID]==\"details\":\n",
    "                return self.classify('number')\n",
    "        return self.classify(sentence)\n",
    "    \n",
    "    def response(self,sentence,userID='construction'):\n",
    "        #get class of users query\n",
    "        results=self.results(sentence,userID)\n",
    "        print(sentence,results)\n",
    "        #store random response to the query\n",
    "        ans=\"\"\n",
    "        if results:\n",
    "            while results:\n",
    "                for i in self.intents['intents']:\n",
    "                    #check if tag == query's class\n",
    "                    if i['tag'] == results[0][0]:\n",
    "\n",
    "                        #if class contains key as \"set\"\n",
    "                        #then store key as userid along with its value in\n",
    "                        #context dictionary\n",
    "                        if 'set' in i and not 'filter' in i:\n",
    "                            context[userID] = i['set']\n",
    "                        #if the tag doesn't have any filter return response\n",
    "                        if not 'filter' in i:\n",
    "                            ans=random.choice(i['responses'])\n",
    "                            print(\"Query:\",sentence)\n",
    "                            print(\"Bot:\",ans)\n",
    "\n",
    "                        #if a class has key as filter then check if context dictionary key's value is same as filter value\n",
    "                        #return the random response\n",
    "                        if userID in context and 'filter' in i and i['filter']==context[userID]:\n",
    "                            if 'set' in i:\n",
    "                                context[userID] = i['set']\n",
    "                            ans=random.choice(i['responses'])\n",
    "                            \n",
    "                results.pop(0)\n",
    "        #if ans contains some value then return response to user's query else return some message\n",
    "        return ans if ans!=\"\" else \"Sorry currently I can't answer this! I am still Learning.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
